# -*- coding: utf-8 -*-
"""ML5_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cvps8MVUnDw_sl8U-9Hu4oztXZ-dAagd

##Методические указания
"""

import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings("ignore")

titanic_data = pd.read_csv('titanic.csv')

titanic_info = titanic_data.info()
titanic_description = titanic_data.describe(include='all')

customer_support_data = pd.read_csv('Customer_support_data.csv')

customer_support_info = customer_support_data.info()
customer_support_description = customer_support_data.describe(include='all')

titanic_info, titanic_description, customer_support_info, customer_support_description

import matplotlib.pyplot as plt
import seaborn as sns

# Определение категориальных признаков и их количества уникальных значений
categorical_columns = customer_support_data.select_dtypes(include=['object']).columns
unique_values = {col: customer_support_data[col].nunique() for col in categorical_columns}

# Функция для создания бар-графиков для категориальных признаков
def plot_categorical_distribution(data, columns):
    plt.figure(figsize=(20, 15))
    for i, col in enumerate(columns, 1):
        plt.subplot(5, 4, i)
        sns.countplot(data=data, y=col, order=data[col].value_counts().index)
        plt.title(f'Distribution of {col}')
        plt.xlabel('Count')
        plt.ylabel(col)
    plt.tight_layout()
    plt.show()

# Фильтрация признаков с небольшим количеством уникальных значений (менее 50)
columns_to_plot = [col for col, unique in unique_values.items() if unique < 50]

plot_categorical_distribution(customer_support_data, columns_to_plot)

"""Основные выводы:
* channel_name: Видно, что большинство обращений поступает через определенные каналы.

* category и Sub-category: Большинство обращений относится к определенным категориям и подкатегориям.

* Agent_name, Supervisor, Manager: Определенные агенты, супервизоры и менеджеры имеют больше обращений, что может говорить о распределении нагрузки.

* Tenure Bucket: Большинство агентов с определенными категориями стажа.

* Agent Shift: Определенные смены агентов более загружены.
"""

from sklearn.preprocessing import LabelEncoder

# Преобразование категориальных признаков в числовые
label_encoders = {}
for column in categorical_columns:
    le = LabelEncoder()
    customer_support_data[column] = le.fit_transform(customer_support_data[column].astype(str))
    label_encoders[column] = le

# Проверка преобразованных данных
customer_support_data.head()

# Расчет корреляции
correlation_matrix = customer_support_data.corr()

# Выбор корреляции с целевой переменной CSAT Score
csat_corr = correlation_matrix["CSAT Score"].sort_values(ascending=False)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

plt.figure(figsize=(20, 15))
for i, col in enumerate(categorical_columns, 1):
    plt.subplot(5, 4, i)
    sns.boxplot(data=customer_support_data, y=col, x="CSAT Score")
    plt.title(f'CSAT Score vs {col}')
plt.tight_layout()
plt.show()

csat_corr

# Укрупнение категорий для channel_name
customer_support_data['channel_name'] = customer_support_data['channel_name'].replace(
    customer_support_data['channel_name'].value_counts()[customer_support_data['channel_name'].value_counts() < 1000].index, 'Other')

# Укрупнение категорий для category
customer_support_data['category'] = customer_support_data['category'].replace(
    customer_support_data['category'].value_counts()[customer_support_data['category'].value_counts() < 1000].index, 'Other')

# Укрупнение категорий для Sub-category
customer_support_data['Sub-category'] = customer_support_data['Sub-category'].replace(
    customer_support_data['Sub-category'].value_counts()[customer_support_data['Sub-category'].value_counts() < 1000].index, 'Other')

# Укрупнение категорий для Product_category
customer_support_data['Product_category'] = customer_support_data['Product_category'].replace(
    customer_support_data['Product_category'].value_counts()[customer_support_data['Product_category'].value_counts() < 1000].index, 'Other')

# Укрупнение категорий для Tenure Bucket
customer_support_data['Tenure Bucket'] = customer_support_data['Tenure Bucket'].replace(
    customer_support_data['Tenure Bucket'].value_counts()[customer_support_data['Tenure Bucket'].value_counts() < 1000].index, 'Other')

# Укрупнение категорий для Agent Shift
customer_support_data['Agent Shift'] = customer_support_data['Agent Shift'].replace(
    customer_support_data['Agent Shift'].value_counts()[customer_support_data['Agent Shift'].value_counts() < 1000].index, 'Other')

# Проверка укрупнения
customer_support_data[categorical_columns].apply(pd.Series.nunique)

plot_categorical_distribution(customer_support_data, columns_to_plot)

"""Укрупнение категорий помогло сократить количество уникальных значений в признаках, что сделало данные более управляемыми и, возможно, улучшит качество модели."""

# Рассчет средней оценки CSAT Score для каждого агента
agent_csat_mean = customer_support_data.groupby('Agent_name')['CSAT Score'].mean().reset_index()
agent_csat_mean.columns = ['Agent_name', 'Agent_CSAT_Mean']

# Добавление нового столбца к основному датасету
customer_support_data = customer_support_data.merge(agent_csat_mean, on='Agent_name', how='left')

# Проверка добавленного столбца
customer_support_data.head()

# Проверка корреляции нового столбца с целевой переменной
new_corr = customer_support_data[['CSAT Score', 'Agent_CSAT_Mean']].corr()

plt.figure(figsize=(10, 6))
sns.scatterplot(data=customer_support_data, x='Agent_CSAT_Mean', y='CSAT Score')
plt.title('CSAT Score vs Agent_CSAT_Mean')
plt.xlabel('Agent_CSAT_Mean')
plt.ylabel('CSAT Score')
plt.show()

new_corr

# Заполнение числовых признаков средним значением
customer_support_data['Item_price'].fillna(customer_support_data['Item_price'].mean(), inplace=True)
customer_support_data['connected_handling_time'].fillna(customer_support_data['connected_handling_time'].mean(), inplace=True)

# Заполнение временных меток специальным значением (например, 0 или 'missing')
customer_support_data['order_date_time'].fillna('missing', inplace=True)
customer_support_data['Issue_reported at'].fillna('missing', inplace=True)
customer_support_data['issue_responded'].fillna('missing', inplace=True)
customer_support_data['Survey_response_Date'].fillna('missing', inplace=True)

# Заполнение категориальных признаков наиболее часто встречающимся значением
categorical_cols = customer_support_data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    customer_support_data[col].fillna(customer_support_data[col].mode()[0], inplace=True)

# Проверка на пропущенные значения
missing_values_after = customer_support_data.isnull().sum()

missing_values_after

titanic_data = pd.read_csv('titanic.csv')

# Заполнение пропущенных значений в категориальных переменных
titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)

# One-Hot Encoding для переменных Sex и Embarked
titanic_data = pd.get_dummies(titanic_data, columns=['Sex', 'Embarked'], drop_first=True)

# Label Encoding для переменной Pclass
titanic_data['Pclass'] = titanic_data['Pclass'].astype('category')
titanic_data['Pclass'] = titanic_data['Pclass'].cat.codes

titanic_data.head()

"""One-Hot Encoding:

Sex был преобразован в Sex_male, где 1 обозначает мужчину, а 0 обозначает женщину.
Embarked был преобразован в два столбца: Embarked_Q и Embarked_S. Значение 1 обозначает, что пассажир отправился из данного порта, а 0 — что нет.

Label Encoding:

Pclass был преобразован в числовые коды (0, 1, 2), где каждое значение соответствует классу билета (1-й, 2-й, и 3-й классы).
"""

customer_support_data = pd.read_csv('Customer_support_data.csv')

columns_to_drop = [
    'Unique id', 'Customer Remarks', 'Order_id', 'order_date_time',
    'Issue_reported at', 'issue_responded', 'Survey_response_Date', 'Customer_City'
]

customer_support_data.drop(columns=columns_to_drop, inplace=True)

# Заполнение пропущенных значений в оставшихся категориальных переменных
categorical_cols = customer_support_data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    customer_support_data[col].fillna(customer_support_data[col].mode()[0], inplace=True)

# Преобразование категориальных переменных с помощью get_dummies()
customer_support_data = pd.get_dummies(customer_support_data, drop_first=True)

customer_support_data.head()

"""##Задания для самостоятельного выполнения

#### 1. Постройте визуализацию распределения признаков и совместного распределения признаков и целевой переменной с помощью тепловых карт. Попробуйте использовать другие инструменты визуализации.
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

customer_support_data = pd.read_csv('Customer_support_data.csv')

columns_to_drop = [
    'Unique id', 'Customer Remarks', 'Order_id', 'order_date_time',
    'Issue_reported at', 'issue_responded', 'Survey_response_Date', 'Customer_City'
]

customer_support_data.drop(columns=columns_to_drop, inplace=True)

# Заполнение пропущенных значений
categorical_cols = customer_support_data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    customer_support_data[col].fillna(customer_support_data[col].mode()[0], inplace=True)

# Преобразование категориальных переменных
customer_support_data = pd.get_dummies(customer_support_data, drop_first=True)

# Выбор числовых признаков
numeric_cols = customer_support_data.select_dtypes(include=['float64', 'int64']).columns

# Выбор первых 10 категориальных признаков для сокращения объема данных
categorical_dummy_cols = customer_support_data.select_dtypes(include=['uint8']).columns[:10]

# Создание сокращенного датасета
reduced_data = customer_support_data[list(numeric_cols) + list(categorical_dummy_cols)]

# Расчет корреляции для сокращенного набора данных
corr_matrix_reduced = reduced_data.corr()

# Построение тепловой карты
plt.figure(figsize=(15, 12))
sns.heatmap(corr_matrix_reduced, annot=False, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap (Reduced)')
plt.show()

# Построение гистограмм для числовых признаков
reduced_data[numeric_cols].hist(figsize=(15, 12), bins=30, edgecolor='black')
plt.suptitle('Distribution of Numerical Features')
plt.show()

# Визуализация совместного распределения признаков и целевой переменной
plt.figure(figsize=(15, 12))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(4, 4, i)
    sns.scatterplot(data=reduced_data, x=col, y='CSAT Score')
    plt.title(f'{col} vs CSAT Score')
plt.tight_layout()
plt.show()

"""#### 2. Постройте на получившимся датасете Customer support модель дерева решений и проанализируйте важность признаков. Сделайте вывод об адекватности наших предположений."""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Загрузка данных Customer Support
customer_support_data = pd.read_csv('Customer_support_data.csv')

# Удаление лишних столбцов
columns_to_drop = [
    'Unique id', 'Customer Remarks', 'Order_id', 'order_date_time',
    'Issue_reported at', 'issue_responded', 'Survey_response_Date', 'Customer_City'
]

customer_support_data.drop(columns=columns_to_drop, inplace=True)

# Заполнение пропущенных значений
for col in customer_support_data.select_dtypes(include=['object']).columns:
    customer_support_data[col].fillna(customer_support_data[col].mode()[0], inplace=True)

for col in customer_support_data.select_dtypes(include=['float64', 'int64']).columns:
    customer_support_data[col].fillna(customer_support_data[col].mean(), inplace=True)

# Преобразование категориальных переменных
customer_support_data = pd.get_dummies(customer_support_data, drop_first=True)

# Разделение данных на признаки и целевую переменную
X = customer_support_data.drop('CSAT Score', axis=1)
y = customer_support_data['CSAT Score']

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Обучение модели дерева решений
model = DecisionTreeRegressor(random_state=42)
model.fit(X_train, y_train)

# Предсказание на тестовых данных
y_pred = model.predict(X_test)

# Оценка модели
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Вывод метрик модели
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

# Оценка важности признаков
feature_importances = model.feature_importances_
features = X.columns

# Визуализация важности признаков
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 8))
sns.barplot(data=importance_df.head(20), x='Importance', y='Feature')
plt.title('Top 20 Important Features')
plt.show()

"""####3. Разбейте датасет на тестовую и обучающую выборки и преобразуйте обе подвыборки. Тестовую нужно преобразовывать точно также, как и обучающую (с теми же параметрами)."""

from sklearn.preprocessing import StandardScaler

file_path = 'Customer_support_data.csv'
data = pd.read_csv(file_path)

data_head = data.head()

train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Определение числовых столбцов
numeric_columns = train_data.select_dtypes(include=['number']).columns

scaler = StandardScaler()

# Обучение стандартизатора и трансформация числовых столбцов обучающей выборки
train_data_scaled = train_data.copy()
train_data_scaled[numeric_columns] = scaler.fit_transform(train_data[numeric_columns])

# Трансформация числовых столбцов тестовой выборки с использованием тех же параметров
test_data_scaled = test_data.copy()
test_data_scaled[numeric_columns] = scaler.transform(test_data[numeric_columns])


(train_data_scaled.head(), test_data_scaled.head())