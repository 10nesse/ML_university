# -*- coding: utf-8 -*-
"""ML5_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cvps8MVUnDw_sl8U-9Hu4oztXZ-dAagd

##Методические указания
"""

import pandas as pd
import numpy as np
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

prsa_data = pd.read_csv("PRSA_Data.csv")
prsa_data.head()

prsa_data.info()

prsa_data.describe()

prsa_data[prsa_data == -1] = np.nan
prsa_data.head()

sns.histplot(prsa_data.SO2)

sns.kdeplot(prsa_data.NO2)

prsa_data.RAIN[prsa_data.RAIN > 0]

is_rain = np.array(prsa_data.RAIN)
is_rain[is_rain > 0] = 1
prsa_data['IS_RAIN'] = is_rain
prsa_data.drop(['RAIN'], axis=1, inplace=True)
prsa_data.describe()

sns.kdeplot(data=prsa_data, x="SO2", hue="AQI Label")

sns.kdeplot(data=prsa_data, x="CO", hue="AQI Label", log_scale=True)

bin_ranges = [0, 250, 320, 10000]
bin_names = [1, 2, 3]
prsa_data['CO_bin_custom_label'] = pd.cut(np.array(prsa_data['CO']),
                                               bins=bin_ranges, labels=bin_names)
prsa_data.head()

prsa_data['CO_bin_custom_label'] = prsa_data['CO_bin_custom_label'].values.add_categories(0)
prsa_data['CO_bin_custom_label'] = prsa_data['CO_bin_custom_label'].fillna(0).astype(int)
prsa_data.head()

prsa_data.PRES[prsa_data.PRES <= 992] = 992
prsa_data.PRES[prsa_data.PRES >= 1034] = 1034

sns.heatmap(prsa_data.isnull(), yticklabels=False, cbar=False)

undef = prsa_data.isnull().sum(axis=1)
undef[undef >= 2]

prsa_data = prsa_data.drop(undef[undef >= 2].index, axis=0)

prsa_data.isnull().sum()

prsa_data.SO2 = prsa_data.SO2.fillna(prsa_data.PRES.mean())

filler = prsa_data.O3[prsa_data.O3.isna()]

filler = prsa_data.O3[~prsa_data.O3.isna()].sample(n=len(filler)).set_axis(filler.index)

prsa_data.O3 = prsa_data.O3.fillna(filler)

prsa_data['O3'] = np.array(np.round((prsa_data['O3'])), dtype='int')
prsa_data.head()

prsa_data.SO2 = np.log(prsa_data.SO2)

"""##Задания для самостоятельного выполнения

#### 1. При выполнении 3 и 5 заданий используйте другие методы визуализации. Найдите самый подходящий тип графика для каждого распределения.
"""

import matplotlib.pyplot as plt

sns.histplot(prsa_data['SO2'].dropna(), kde=False)
plt.title('Histogram of SO2')
plt.xlabel('SO2')
plt.ylabel('Frequency')
plt.show()

sns.kdeplot(prsa_data['NO2'].dropna())
plt.title('Density Plot of NO2')
plt.xlabel('NO2')
plt.ylabel('Density')
plt.show()

sns.boxplot(y=prsa_data['CO'].dropna())
plt.title('Box Plot of CO')
plt.ylabel('CO')
plt.show()

sns.violinplot(y=prsa_data['O3'].dropna())
plt.title('Violin Plot of O3')
plt.ylabel('O3')
plt.show()

"""#### 2. При выполнении 6 задания мы явно подбирали руками границы диапазона для клиппинга. Реализуйте адаптивный клиппинг через процентили."""

# Функция для адаптивного клиппинга через процентили
def adaptive_clipping(df, columns, lower_percentile=1, upper_percentile=99):
    for col in columns:
        lower_bound = np.percentile(df[col].dropna(), lower_percentile)
        upper_bound = np.percentile(df[col].dropna(), upper_percentile)
        df[col] = np.clip(df[col], lower_bound, upper_bound)
    return df

# Определим численные столбцы
numeric_columns = ['SO2', 'NO2', 'CO', 'O3', 'PRES', 'WSPM']

# Применим адаптивный клиппинг к датасету
prsa_data_clipped = adaptive_clipping(prsa_data.copy(), numeric_columns)

# Выведем описательную статистику для проверки результатов
prsa_data_clipped.describe()

"""Адаптивный клиппинг через процентили успешно выполнен. Описательная статистика после клиппинга показывает, что экстремальные значения были ограничены 1-м и 99-м процентилями для каждого численного признака. Например, максимальные значения для признаков SO2, NO2, CO, O3, PRES, и WSPM стали значительно ниже, чем до клиппинга."""

sns.histplot(prsa_data_clipped['SO2'].dropna(), kde=False)
plt.title('Histogram of SO2 after Clipping')
plt.xlabel('SO2')
plt.ylabel('Frequency')
plt.show()

sns.kdeplot(prsa_data_clipped['NO2'].dropna())
plt.title('Density Plot of NO2 after Clipping')
plt.xlabel('NO2')
plt.ylabel('Density')
plt.show()

"""Результаты показывают, что после клиппинга распределения численных признаков стали более ограниченными, что поможет уменьшить влияние выбросов на модель машинного обучения.

####3. Избавьтесь от оставшихся пропусков в данных. Самостоятельно выберите метод.
"""

# Заполнение пропусков медианой для каждого численного признака
prsa_data_filled = prsa_data_clipped.copy()
for col in numeric_columns:
    prsa_data_filled[col] = prsa_data_filled[col].fillna(prsa_data_filled[col].median())

# Проверим, что пропусков больше нет
missing_values = prsa_data_filled.isnull().sum()

filled_description = prsa_data_filled.describe()

missing_values, filled_description

"""Пропуски успешно заполнены медианой для всех численных признаков, за исключением столбца RAIN, который содержит 20 пропущенных значений."""

# Заполнение пропусков медианой для столбца RAIN
prsa_data_filled['RAIN'] = prsa_data_filled['RAIN'].fillna(prsa_data_filled['RAIN'].median())

# Проверим, что пропусков больше нет
missing_values_after_rain_fill = prsa_data_filled.isnull().sum()

final_description = prsa_data_filled.describe()

missing_values_after_rain_fill, final_description

"""Теперь все пропуски в данных успешно заполнены, и пропусков больше нет. Вот сводка по датасету после обработки:

####4. Проведите нормализацию численных признаков. Выберите наиболее подходящий вид нормализации для каждого признака.
"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
import numpy as np

prsa_data_normalized = prsa_data_filled.copy()

numeric_columns = ['SO2', 'NO2', 'CO', 'O3', 'PRES', 'WSPM']

# Применим стандартную нормализацию для признаков с нормальным распределением
standard_scaler = StandardScaler()
prsa_data_normalized[['PRES', 'WSPM']] = standard_scaler.fit_transform(prsa_data_normalized[['PRES', 'WSPM']])

# Применим минимаксную нормализацию для признаков с распределением в широком диапазоне
minmax_scaler = MinMaxScaler()
prsa_data_normalized[['SO2', 'NO2', 'O3']] = minmax_scaler.fit_transform(prsa_data_normalized[['SO2', 'NO2', 'O3']])

# Применим нормализацию методом Робуста для признаков с выбросами
robust_scaler = RobustScaler()
prsa_data_normalized[['CO']] = robust_scaler.fit_transform(prsa_data_normalized[['CO']])

# Применим логарифмическую нормализацию для признаков с большой асимметрией (если необходимо)
prsa_data_normalized['CO'] = np.log1p(prsa_data_normalized['CO'])


normalized_description = prsa_data_normalized.describe()
normalized_description

"""####5. Постройте кореллограмму по всем численным столбцам датасета. Сделайте вывод о значимости признаков."""

correlation_matrix = prsa_data_normalized[numeric_columns].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""####6. Визуализируйте связи между признаками. Сделайте вывод об их взаимозависимости."""

from scipy.cluster import hierarchy

correlation_matrix = prsa_data_normalized[numeric_columns].corr()

distance_matrix = 1 - np.abs(correlation_matrix)

Z = hierarchy.linkage(distance_matrix, method='average')

plt.figure(figsize=(10, 8))
hierarchy.dendrogram(Z, labels=numeric_columns, leaf_rotation=90)
plt.title('Dendrogram of Numerical Features')
plt.xlabel('Features')
plt.ylabel('Distance')
plt.show()