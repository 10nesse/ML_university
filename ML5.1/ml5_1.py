# -*- coding: utf-8 -*-
"""ML5_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_EKLlRHW7cPp46x0-rMkR5_EyM_TYTJ

##Методические указания
"""

import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings("ignore")

data1 = pd.read_csv('issues1.csv', index_col=0)
data1.head()

data2 = pd.read_csv('issues2.csv', index_col=0)
data2

data3 = pd.read_csv('issues4.csv', index_col=0)
data3.head()

data1.shape, data2.shape, data3.shape

pd.concat([data1, data2]).head()

data1.info()

data2.info()

data2 = data2.rename(columns={
    'channel_name': 'Channel',
    'Reported at': 'Issue_reported_Date',
    'Responded at': 'Issue_responded_Date',
    'Survey responded at': 'Survey_response_Date',
})

data2['Issue_reported_Date'] = pd.to_datetime(data2['Issue_reported_Date'])
data2['Issue_responded_Date'] = pd.to_datetime(data2['Issue_responded_Date'])
data2['Survey_response_Date'] = pd.to_datetime(data2['Survey_response_Date'], format='%d %m %Y')

data2.head()

data1['Issue_reported_Date'] = pd.to_datetime(data1['Issue_reported_Date'])
data1['Issue_responded_Date'] = pd.to_datetime(data1['Issue_responded_Date'])
data1['Survey_response_Date'] = pd.to_datetime(data1['Survey_response_Date'], format='%d-%b-%y')

data1.info()

data_12 = pd.concat([data1, data2])
data_12.head()

data_12.info()

data3.info()

data3 = data3.rename(columns={
    'Unique id': 'Id',
    'channel_name': 'Channel',
    'Issue_reported at': 'Issue_reported_Date',
    'issue_responded': 'Issue_responded_Date',
    'Survey responded at': 'Survey_response_Date',
    'Agent': 'Agent_name'
})

data3['Issue_reported_Date'] = pd.to_datetime(data3['Issue_reported_Date'])
data3['Issue_responded_Date'] = pd.to_datetime(data3['Issue_responded_Date'])
data3['Survey_response_Date'] = pd.to_datetime(data3['Survey_response_Date'])

data3.head()

data3.info()

data_123 = pd.concat([data_12, data3])
data_123.head()

data_123.info()

data_123.head()

data3['Agent_name'] = data3['Agent_name'].apply(
    lambda x: x.split()[0][0] + ". " + x.split()[1] if isinstance(x, str) else np.NAN
)

data_123 = pd.concat([data_12, data3])

orders_data = pd.read_csv('orders.csv', index_col=0)
orders_data.head()

orders_data = orders_data.rename(columns={
    'Id': 'Order_id',
})

data_with_orders = data_123.merge(orders_data, on='Order_id', how='left')
data_with_orders.head()

orders_data[orders_data.Order_id == 'a5464619-b2d8-49de-9346-5e5db0972756']

import seaborn as sns

sns.heatmap(data_with_orders.isnull(), yticklabels=False, cbar=False)

agent_data = pd.read_csv("agents.csv", index_col=0)
agent_data.head()

agent_data.groupby(['Agent']).agg('count').head(20)

agent_data['Agent_name'] = agent_data['Agent'].apply(
    lambda x: x.split()[0][0] + ". " + x.split()[1] if isinstance(x, str) else np.NAN
)

agent_data.groupby(['Agent_name']).agg('first').head(20)

data_123.groupby(['Agent_name']).agg('count').head(20)

data_with_agents = data_with_orders.join(agent_data, rsuffix='_1', how='left')
data_with_agents.head()

sns.heatmap(data_with_agents.isnull(), yticklabels=False, cbar=False)

data_with_agents = data_with_orders.merge(agent_data, how='left', on='Agent_name', copy=False)
data_with_agents.head()

sns.heatmap(data_with_agents.isnull(), yticklabels=False, cbar=False)

agent_data.Agent_name.value_counts()

agent_data[agent_data.Agent_name == 'J. Moore']

data_with_agent_numbers = data_with_orders.merge(agent_data.Agent_name.value_counts(), how='left', left_on='Agent_name', right_index=True)

"""##Задания для самостоятельного выполнения

#### 1. При выполнении вертикального объединения убедитесь в отсутствии дубликатов данных.
"""

import pandas as pd

data_issue1 = pd.read_csv('https://raw.githubusercontent.com/koroteevmv/ML_course/main/ML5.1%20data%20integration/data/issues1.csv', index_col=0)
data_issue2 = pd.read_csv('https://raw.githubusercontent.com/koroteevmv/ML_course/main/ML5.1%20data%20integration/data/issues2.csv', index_col=0)

data_issue2 = data_issue2.rename(columns={
    'channel_name': 'Channel',
    'Reported at': 'Issue_reported_Date',
    'Responded at': 'Issue_responded_Date',
    'Survey responded at': 'Survey_response_Date',
})

data_issue2['Issue_reported_Date'] = pd.to_datetime(data_issue2['Issue_reported_Date'])
data_issue2['Issue_responded_Date'] = pd.to_datetime(data_issue2['Issue_responded_Date'])
data_issue2['Survey_response_Date'] = pd.to_datetime(data_issue2['Survey_response_Date'], format='%d %m %Y')

data_issue1['Issue_reported_Date'] = pd.to_datetime(data_issue1['Issue_reported_Date'], format='%d/%m/%Y %H:%M')
data_issue1['Issue_responded_Date'] = pd.to_datetime(data_issue1['Issue_responded_Date'], format='%d/%m/%Y %H:%M')
data_issue1['Survey_response_Date'] = pd.to_datetime(data_issue1['Survey_response_Date'], format='%d-%b-%y')

merged_data = pd.concat([data_issue1, data_issue2], ignore_index=True)

duplicate_rows = merged_data[merged_data.duplicated()]

"""#### 2. При горизонтальной интеграции первой и второй таблицы выведите по каждому столбцу гистограмму распределения или плотность распределения, чтобы убедиться, что признаки выражаются по соотносимым шкалам.

Можно заметить, что по оси Ox они имеют одинаковые размерности.
"""

data2.plot.hist()

data1.plot.hist()

"""####3. При выполнении третьего задания убедитесь более явно, что обильное количество пропущенных значений не является ошибкой объединения. Проведите больше, чем одну точечную проверку."""

orders_data = pd.read_csv('https://raw.githubusercontent.com/koroteevmv/ML_course/main/ML5.1%20data%20integration/data/orders.csv', index_col=0)
orders_data = orders_data.rename(columns={
    'Id': 'Order_id',
})

orders_data.head()

data_123.head()

sns.heatmap(data_123[['Order_id']].isnull(), yticklabels=False, cbar=False)

data_with_orders = data_123.merge(orders_data, on='Order_id', how='left')
data_with_orders.head()

sns.heatmap(data_with_orders[['Order_id']].isnull(), yticklabels=False, cbar=False)

"""Мы видим, что диаграммы практически идентичны, что свидетельствует о том, что данные не были потеряны в процессе объединения таблиц.

####4. При выполнении третьего задания попробуйте использовать разные виды соединений. Как это отражается на структуре датасета?
"""

data_with_orders_1 = data_123.merge(orders_data, on='Order_id', how='left')
sns.heatmap(data_with_orders_1.isnull(), yticklabels=False, cbar=False)

data_with_orders_2 = data_123.merge(orders_data, on='Order_id', how='right')
sns.heatmap(data_with_orders_2.isnull(), yticklabels=False, cbar=False)

data_with_orders_3 = data_123.merge(orders_data, on='Order_id', how='inner')
sns.heatmap(data_with_orders_3.isnull(), yticklabels=False, cbar=False)

data_with_orders_4 = data_123.merge(orders_data, on='Order_id', how='outer')
sns.heatmap(data_with_orders_4.isnull(), yticklabels=False, cbar=False)

"""В результате разных типов соединения мы наблюдаем различное распределение пустых значений в итоговом датасете.






"""